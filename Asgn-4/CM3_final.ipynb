{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5d1e64b",
   "metadata": {},
   "source": [
    "<h2><center>ASSIGNMENT 4</center></h2>\n",
    "<h2><center>DEEP ASHISH JARIWALA, JAYDEEP SINGH</center></h2>\n",
    "<h2><center>GROUP - 41</center></h2>\n",
    "<h2><center>Q: CM3</center></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351c7ad0",
   "metadata": {},
   "source": [
    "<p>The following notebook provides a detailed comparsion of all the algorithms (with run time performance) applied to classify the COVID-19 dataset. The following steps were followed to select the best model:</p>\n",
    "<ul>\n",
    "            <li>Testing different architecture for neural networks and selecting the best performing model in terms of accuracy and computation complexity</li>\n",
    "            <li>The selected model has many hyperparamaters like optimizers,activation functions and learning rate that still needs tuning to get the best performing model</li>\n",
    "            <li>The best selected model in terms of architecture is tuned for 6 different activation functions to get the best result</li>\n",
    "            <li>Then, the best tuned model from the above case is tuned for 6 different optimizers to get the best performing result</li>\n",
    "            <li>Once the optimizer and activation functions are selected the model is compared using different learning rates and regularization parameter to get the final model for classification</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ddcee2d",
   "metadata": {},
   "source": [
    "# SELECTION OF BEST ARCHITECTURE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b064e848",
   "metadata": {},
   "source": [
    "### MODEL 1 GRAPHS AND SUMMARY (BEST PERFOMING MODEL USED IN CM1)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b592e23c",
   "metadata": {},
   "source": [
    "Model: \"MODEL_1\"\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "dense_42 (Dense)             (None, 112)               5824      \n",
    "_________________________________________________________________\n",
    "dropout_15 (Dropout)         (None, 112)               0         \n",
    "_________________________________________________________________\n",
    "dense_43 (Dense)             (None, 448)               50624     \n",
    "_________________________________________________________________\n",
    "dense_44 (Dense)             (None, 56)                25144     \n",
    "_________________________________________________________________\n",
    "dense_45 (Dense)             (None, 3)                 171       \n",
    "=================================================================\n",
    "Total params: 81,763\n",
    "Trainable params: 81,763\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d99a0e",
   "metadata": {},
   "source": [
    "<img src = 'final_model1.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3da196",
   "metadata": {},
   "source": [
    "<p>Overall, this is the best performing model that provides the best accuracy. It can be seen that validation line for accuracy achieves stability around 40 epochs. We have used early stopping (patiance of 15) as a part of training our model in order to determine the number of epochs required to have an accurate fit for the classification model. This is a less complex architecture defined with appropriate hyperparameters (tested below) and regularization techniques to get the best results. </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f540d3",
   "metadata": {},
   "source": [
    "### MODEL 2 GRAPHS AND SUMMARY"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6da7e4fc",
   "metadata": {},
   "source": [
    "Model: \"MODEL_2\"\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "dense_46 (Dense)             (None, 112)               5824      \n",
    "_________________________________________________________________\n",
    "dropout_16 (Dropout)         (None, 112)               0         \n",
    "_________________________________________________________________\n",
    "dense_47 (Dense)             (None, 1120)              126560    \n",
    "_________________________________________________________________\n",
    "dropout_17 (Dropout)         (None, 1120)              0         \n",
    "_________________________________________________________________\n",
    "dense_48 (Dense)             (None, 1120)              1255520   \n",
    "_________________________________________________________________\n",
    "dense_49 (Dense)             (None, 448)               502208    \n",
    "_________________________________________________________________\n",
    "dropout_18 (Dropout)         (None, 448)               0         \n",
    "_________________________________________________________________\n",
    "dense_50 (Dense)             (None, 448)               201152    \n",
    "_________________________________________________________________\n",
    "dense_51 (Dense)             (None, 112)               50288     \n",
    "_________________________________________________________________\n",
    "dropout_19 (Dropout)         (None, 112)               0         \n",
    "_________________________________________________________________\n",
    "dense_52 (Dense)             (None, 56)                6328      \n",
    "_________________________________________________________________\n",
    "dense_53 (Dense)             (None, 3)                 171       \n",
    "=================================================================\n",
    "Total params: 2,148,051\n",
    "Trainable params: 2,148,051\n",
    "Non-trainable params: 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f522993",
   "metadata": {},
   "source": [
    "<img src = 'model2.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d53275",
   "metadata": {},
   "source": [
    "<p>Overall, this is the good performing model but has performance metrics less than the best model. It can be seen that validation line for accuracy achieves stability around 35 epochs. We have used early stopping (patiance of 15) as a part of training our model in order to determine the number of epochs required to have an accurate fit for the classification model. This is a complex architecture with a greater depth and higher number of training parameters. This model is neglected due to the fact that it provides comparable performance to a less complex and dense network. Moreover, it takes a long time to train due to a high computational complexity.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "939209ad",
   "metadata": {},
   "source": [
    "### MODEL 3 GRAPHS AND SUMMARY"
   ]
  },
  {
   "cell_type": "raw",
   "id": "18963a17",
   "metadata": {},
   "source": [
    "Model: \"MODEL_3\"\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "dense_54 (Dense)             (None, 448)               23296     \n",
    "_________________________________________________________________\n",
    "dense_55 (Dense)             (None, 112)               50288     \n",
    "_________________________________________________________________\n",
    "dense_56 (Dense)             (None, 3)                 339       \n",
    "=================================================================\n",
    "Total params: 73,923\n",
    "Trainable params: 73,923\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c56f4ff",
   "metadata": {},
   "source": [
    "<img src = 'model3.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc42656",
   "metadata": {},
   "source": [
    "<p>This model provides a very simple architecture and has achieved a good stability compared to other models. This model is neglected due to the fact that it lacks regularization technique for getting better performance in terms of accuracy and minimizing the overall loss. This networks has very less parameters and has performance comparable to other dense models. Moreover, adding regularization in this network achieved the best performing model.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b815853",
   "metadata": {},
   "source": [
    "### MODEL 4 GRAPHS AND SUMMARY"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bf3c78bf",
   "metadata": {},
   "source": [
    "Model: \"MODEL_4\"\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "dense_57 (Dense)             (None, 112)               5824      \n",
    "_________________________________________________________________\n",
    "dropout_20 (Dropout)         (None, 112)               0         \n",
    "_________________________________________________________________\n",
    "dense_58 (Dense)             (None, 1120)              126560    \n",
    "_________________________________________________________________\n",
    "dropout_21 (Dropout)         (None, 1120)              0         \n",
    "_________________________________________________________________\n",
    "dense_59 (Dense)             (None, 1120)              1255520   \n",
    "_________________________________________________________________\n",
    "dense_60 (Dense)             (None, 448)               502208    \n",
    "_________________________________________________________________\n",
    "dropout_22 (Dropout)         (None, 448)               0         \n",
    "_________________________________________________________________\n",
    "dense_61 (Dense)             (None, 448)               201152    \n",
    "_________________________________________________________________\n",
    "dense_62 (Dense)             (None, 448)               201152    \n",
    "_________________________________________________________________\n",
    "dropout_23 (Dropout)         (None, 448)               0         \n",
    "_________________________________________________________________\n",
    "dense_63 (Dense)             (None, 112)               50288     \n",
    "_________________________________________________________________\n",
    "dense_64 (Dense)             (None, 112)               12656     \n",
    "_________________________________________________________________\n",
    "dropout_24 (Dropout)         (None, 112)               0         \n",
    "_________________________________________________________________\n",
    "dense_65 (Dense)             (None, 56)                6328      \n",
    "_________________________________________________________________\n",
    "dense_66 (Dense)             (None, 27)                1539      \n",
    "_________________________________________________________________\n",
    "dropout_25 (Dropout)         (None, 27)                0         \n",
    "_________________________________________________________________\n",
    "dense_67 (Dense)             (None, 3)                 84        \n",
    "=================================================================\n",
    "Total params: 2,363,311\n",
    "Trainable params: 2,363,311\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5b1ff5",
   "metadata": {},
   "source": [
    "<img src = 'model4.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b91e1f9f",
   "metadata": {},
   "source": [
    "<p>This is a very complex model with 11 layers with many regularization layers (drop out and L2 norm). It can be seen that the training of the model is stable with a good run time performance but it is computationally very complex and time taking. Furthermore, the results obtained using this case has provided lesser or comparable results to a less complex model in terms of classification. Therefore, this model is neglected</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aac39e7",
   "metadata": {},
   "source": [
    "<p>The table below shows the performance metrics for all the architectures of neural networks. The dataset used was splitted in 75-15-10% ratio. 75% of the data is used for training, 15% validation and 10% completely unknown test set. The following results were obtained by different architecture:</p>\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "    <th>MODEL</th>\n",
    "    <th>LAYERS</th>\n",
    "    <th>PARAMETERS</th>\n",
    "    <th>REGULARIZATION</th>\n",
    "    <th>OPTIMIZER and ACTIVATION</th>\n",
    "    <th>MEAN TRAINNG ACCURACY</th>\n",
    "    <th>MEAN TRAINNG LOSS</th>\n",
    "    <th>MEAN VALIDATION ACCURACY</th>\n",
    "    <th>MEAN VALIDATION LOSS</th>\n",
    "    <th>TESTING ACCURACY</th>\n",
    "    <th>COMPUTATION COMPLEXITY</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>1</td>\n",
    "    <td>3</td>\n",
    "    <td>81,763</td>\n",
    "    <td>L2norm and dropout(0.07)</td>\n",
    "    <td>ADAM and RELU</td>\n",
    "    <td>90.82%</td>\n",
    "    <td>26.51%</td>\n",
    "    <td>89.76%</td>\n",
    "    <td>28.77%</td>\n",
    "    <td>91.67%</td>\n",
    "    <td>37 epochs - 18s</td>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <td>2</td>\n",
    "    <td>8</td>\n",
    "    <td>2,148,051</td>\n",
    "    <td>L2 norm and 4-dropout(0.15)</td>\n",
    "    <td>ADAM and RELU</td>\n",
    "    <td>90.30%</td>\n",
    "    <td>29.77%</td>\n",
    "    <td>90.19%</td>\n",
    "    <td>29.20%</td>\n",
    "    <td>90.17%</td>\n",
    "    <td>36 epochs - 234s</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>3</td>\n",
    "    <td>3</td>\n",
    "    <td>73,923</td>\n",
    "    <td>None</td>\n",
    "    <td>ADAM and RELU</td>\n",
    "    <td>90.73%</td>\n",
    "    <td>26.01%</td>\n",
    "    <td>90.37%</td>\n",
    "    <td>26.07%</td>\n",
    "    <td>89.76%</td>\n",
    "    <td>42 epochs - 21s</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>4</td>\n",
    "    <td>11</td>\n",
    "    <td>2,363,311</td>\n",
    "    <td>L2 norm and 6-dropout(0.15)</td>\n",
    "    <td>ADAM and RELU</td>\n",
    "    <td>89.66%</td>\n",
    "    <td>29.72%</td>\n",
    "    <td>90.05%</td>\n",
    "    <td>27.96%</td>\n",
    "    <td>89.96%</td>\n",
    "    <td>40 epochs - 280s</td>\n",
    "  </tr>\n",
    "    </table>\n",
    "<p>Based on the results it can be seen that model one has a better performance in terms of all the metrics. It is computationally less complex with a comparable or greater accuracy (comparable or less loss) than other deep neural network models with more parameters. The model also has regularization parameters to avoid over fitting. The best way to improve the selected architecture is by tuning various hyperparamters.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b6760f",
   "metadata": {},
   "source": [
    "# SELECTION OF BEST ACTIVATION FUNCTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70bfcdd3",
   "metadata": {},
   "source": [
    "<p>The table below shows the change in performance metrics with respect to activation functions for the best model selected above (MODEL_1).</p>\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "    <th>MODEL</th>\n",
    "    <th>ACTIVATION</th>\n",
    "    <th>MEAN TRAIN ACCURACY</th>\n",
    "    <th>MEAN TRAIN LOSS</th>\n",
    "    <th>MEAN VALIDATION ACCURACY</th>\n",
    "    <th>MEAN VALIDATION LOSS</th>\n",
    "    <th>EPOCHS</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>1</td>\n",
    "    <td>relu</td>\n",
    "    <td>90.51%</td>\n",
    "    <td>28.13%</td>\n",
    "    <td>90.13%</td>\n",
    "    <td>28.13%</td>\n",
    "    <td>42</td>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <td>2</td>\n",
    "    <td>sigmoid</td>\n",
    "    <td>88.11%</td>\n",
    "    <td>35.02%</td>\n",
    "    <td>87.97%</td>\n",
    "    <td>34.52%</td>\n",
    "    <td>32</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>3</td>\n",
    "    <td>tanh</td>\n",
    "    <td>90.65%</td>\n",
    "    <td>27.73%</td>\n",
    "    <td>90.57%</td>\n",
    "    <td>27.45%</td>\n",
    "    <td>45</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>4</td>\n",
    "    <td>leaky_relu</td>\n",
    "    <td>90.53%</td>\n",
    "    <td>28.28%</td>\n",
    "    <td>90.14%</td>\n",
    "    <td>28.55%</td>\n",
    "    <td>26</td>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <td>5</td>\n",
    "    <td>elu</td>\n",
    "    <td>90.66%</td>\n",
    "    <td>27.81%</td>\n",
    "    <td>90.49%</td>\n",
    "    <td>27.39%</td>\n",
    "    <td>42</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>6</td>\n",
    "    <td>selu</td>\n",
    "    <td>90.55%</td>\n",
    "    <td>30.20%</td>\n",
    "    <td>90.42%</td>\n",
    "    <td>29.70%</td>\n",
    "    <td>42</td>\n",
    "  </tr>\n",
    "    </table>\n",
    "<p></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c19d97",
   "metadata": {},
   "source": [
    "<p> It can be seen that activations like relu has out perform other algorithms in terms of performance and computational complextity. A general problem with both the sigmoid and tanh functions is saturation meaning that large values snap to 1.0 and small values snap to -1 or 0 for tanh and sigmoid respectively. Furthermore, the functions are only really sensitive to changes around their mid-point of their input, such as 0.5 for sigmoid and 0.0 for tanh. Therefore, due to the limited sensitivity and saturation of the function happen regardless of whether the summed activation from the node provided as input contains useful information or not. Once saturated, it becomes challenging for the learning algorithm to continue to adapt the weights to improve the performance of the model.</p>\n",
    "<p>Relu rectifies the problem of vanishing gradiets and has many advantages over other metrics like (computational simplicity, representational sparsity, linear behaviour, able to train deep networks). All the statements can be justified using the performance in the classification problem.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb8813e",
   "metadata": {},
   "source": [
    "# SELECTION OF BEST OPTIMIZER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "392a8974",
   "metadata": {},
   "source": [
    "<p>The table below shows the change in performance metrics with respect to optimizers for the best model selected above (MODEL_1).</p>\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "    <th>MODEL</th>\n",
    "    <th>OPTIMIZER</th>\n",
    "    <th>MEAN TRAIN ACCURACY</th>\n",
    "    <th>MEAN TRAIN LOSS</th>\n",
    "    <th>MEAN VALIDATION ACCURACY</th>\n",
    "    <th>MEAN VALIDATION LOSS</th>\n",
    "    <th>EPOCHS</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>1</td>\n",
    "    <td>Adam</td>\n",
    "    <td>91.04%</td>\n",
    "    <td>26.11%</td>\n",
    "    <td>91.04%</td>\n",
    "    <td>25.17%</td>\n",
    "    <td>42</td>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <td>2</td>\n",
    "    <td>RMSProp</td>\n",
    "    <td>89.68%</td>\n",
    "    <td>30.48%</td>\n",
    "    <td>89.07%</td>\n",
    "    <td>30.75%</td>\n",
    "    <td>24</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>3</td>\n",
    "    <td>SGD</td>\n",
    "    <td>89.48%</td>\n",
    "    <td>41.48%</td>\n",
    "    <td>88.94%</td>\n",
    "    <td>42.47%</td>\n",
    "    <td>58</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>4</td>\n",
    "    <td>Adamax</td>\n",
    "    <td>90.53%</td>\n",
    "    <td>28.90%</td>\n",
    "    <td>90.97%</td>\n",
    "    <td>26.57%</td>\n",
    "    <td>56</td>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <td>5</td>\n",
    "    <td>Adagrad</td>\n",
    "    <td>88.79%</td>\n",
    "    <td>41.79%</td>\n",
    "    <td>90.22%</td>\n",
    "    <td>38.06%</td>\n",
    "    <td>400</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>6</td>\n",
    "    <td>Ftrl</td>\n",
    "    <td>33.7%</td>\n",
    "    <td>109.85%</td>\n",
    "    <td>34.42%</td>\n",
    "    <td>109.85%</td>\n",
    "    <td>20</td>\n",
    "  </tr>\n",
    "    </table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "001f611d",
   "metadata": {},
   "source": [
    "<p>Optimizers are the algorithms applied while training the network in order to minimize the loss and reach a minimum. The above table shows a brief comparision of our model at different optimization algorithms. It can be seen that adam optimizer has the best performance among other optimizers. The major advantage observed in case of adam optimizer is the minimization of training and validation loss, in case of other the loss greater than 30% but adam optimizer has out performed other algorithms. </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f15d661",
   "metadata": {},
   "source": [
    "# COMPARING DIFFERENT LEARNING RATES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09479e63",
   "metadata": {},
   "source": [
    "<p>The table below shows the change in performance metrics with respect to learning rate in adam optimizer for the best model selected above (MODEL_1).</p>\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "    <th>MODEL</th>\n",
    "    <th>LEARNING RATE</th>\n",
    "    <th>MEAN TRAIN ACCURACY</th>\n",
    "    <th>MEAN TRAIN LOSS</th>\n",
    "    <th>MEAN VALIDATION ACCURACY</th>\n",
    "    <th>MEAN VALIDATION LOSS</th>\n",
    "    <th>EPOCHS</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>1</td>\n",
    "    <td>0.001</td>\n",
    "    <td>91.42%</td>\n",
    "    <td>24.52%</td>\n",
    "    <td>91.55%</td>\n",
    "    <td>25.83%</td>\n",
    "    <td>21</td>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <td>2</td>\n",
    "    <td>0.005</td>\n",
    "    <td>91.53%</td>\n",
    "    <td>23.88%</td>\n",
    "    <td>91.71%</td>\n",
    "    <td>26.19%</td>\n",
    "    <td>34</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>3</td>\n",
    "    <td>0.01</td>\n",
    "    <td>91.46%</td>\n",
    "    <td>24.46%</td>\n",
    "    <td>91.45%</td>\n",
    "    <td>25.89%</td>\n",
    "    <td>42</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>4</td>\n",
    "    <td>0.05</td>\n",
    "    <td>88.08%</td>\n",
    "    <td>32.63%</td>\n",
    "    <td>91.15%</td>\n",
    "    <td>28.11%</td>\n",
    "    <td>18</td>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <td>5</td>\n",
    "    <td>0.1</td>\n",
    "    <td>86.57%</td>\n",
    "    <td>42.27%</td>\n",
    "    <td>90.04%</td>\n",
    "    <td>36.02%</td>\n",
    "    <td>12</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>6</td>\n",
    "    <td>0.2</td>\n",
    "    <td>34.56%</td>\n",
    "    <td>109.85%</td>\n",
    "    <td>34.04%</td>\n",
    "    <td>109.85%</td>\n",
    "    <td>13</td>\n",
    "  </tr>\n",
    "    </table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80bd76e4",
   "metadata": {},
   "source": [
    "<img src = 'learning_rate_graphs.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7794ec7",
   "metadata": {},
   "source": [
    "<p>The graph shows the variation of learning rate with respect various performance metrics. The learning rate decides the change in model with respect to estimated error each time the model updates the weight. It is a common obervation that higher learning rate causes unstable training of the model with high loss. A proper selection of learning rate can help the algorithm converge and undergo the optimization process easily. It can be observed that the model shows the best performace at lr = 0.001 and starts decreasing rapidly as the learning rate goes beyond 0.1. </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af299b4",
   "metadata": {},
   "source": [
    "# COMPARING DIFFERENT DROP OUT RATES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7fc10e",
   "metadata": {},
   "source": [
    "<img src = 'dr2.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a21405",
   "metadata": {},
   "source": [
    "<p>The graph shows variation of drop out rate on the the best model architecture. It can be seen that the drop rate provides the best results below 0.15, beyond that value the error increases rapidly and causes underfitting to get minimum accuracy. The best performance is seen for the drop rate between 0.05 and 0.1. The best performing model has a drop rate of 0.07 as the selected hyper parameter to finalize the model.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59df3793",
   "metadata": {},
   "source": [
    "<p>The results for the best performing model:</p>\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "    <th>MODEL</th>\n",
    "    <th>LAYERS</th>\n",
    "    <th>PARAMETERS</th>\n",
    "    <th>REGULARIZATION</th>\n",
    "    <th>OPTIMIZER and ACTIVATION</th>\n",
    "    <th>MEAN TRAINNG ACCURACY</th>\n",
    "    <th>MEAN TRAINNG LOSS</th>\n",
    "    <th>MEAN VALIDATION ACCURACY</th>\n",
    "    <th>MEAN VALIDATION LOSS</th>\n",
    "    <th>TESTING ACCURACY</th>\n",
    "    <th>COMPUTATION COMPLEXITY</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>1</td>\n",
    "    <td>3</td>\n",
    "    <td>81,763</td>\n",
    "    <td>L2norm and dropout(0.07)</td>\n",
    "    <td>ADAM and RELU</td>\n",
    "    <td>90.82%</td>\n",
    "    <td>26.51%</td>\n",
    "    <td>89.76%</td>\n",
    "    <td>28.77%</td>\n",
    "    <td>91.67%</td>\n",
    "    <td>37 epochs - 18s</td>\n",
    "  </tr>\n",
    "</table>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
